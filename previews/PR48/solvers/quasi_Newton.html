<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quasi-Newton · Manopt.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Manopt.jl</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><a class="tocitem" href="../about.html">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../tutorials/MeanAndMedian.html">get Started: Optimize!</a></li><li><a class="tocitem" href="../tutorials/StochasticGradientDescent.html">do stochastic gradient descent</a></li><li><a class="tocitem" href="../tutorials/BezierCurves.html">work with Bézier curves</a></li><li><a class="tocitem" href="../tutorials/GradientOfSecondOrderDifference.html">see the gradient of <span>$d_2$</span></a></li><li><a class="tocitem" href="../tutorials/JacobiFields.html">use Jacobi Fields</a></li></ul></li><li><a class="tocitem" href="../plans/index.html">Plans</a></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="ChambollePock.html">Chambolle-Pock</a></li><li><a class="tocitem" href="conjugate_gradient_descent.html">Conjugate gradient descent</a></li><li><a class="tocitem" href="cyclic_proximal_point.html">Cyclic Proximal Point</a></li><li><a class="tocitem" href="DouglasRachford.html">Douglas–Rachford</a></li><li><a class="tocitem" href="gradient_descent.html">Gradient Descent</a></li><li><a class="tocitem" href="NelderMead.html">Nelder–Mead</a></li><li><a class="tocitem" href="particle_swarm.html">Particle Swarm Optimization</a></li><li class="is-active"><a class="tocitem" href="quasi_Newton.html">Quasi-Newton</a><ul class="internal"><li><a class="tocitem" href="#Operator-Updates-1"><span>Operator Updates</span></a></li><li><a class="tocitem" href="#Initialization-1"><span>Initialization</span></a></li><li><a class="tocitem" href="#Iteration-1"><span>Iteration</span></a></li><li><a class="tocitem" href="#Result-1"><span>Result</span></a></li><li><a class="tocitem" href="#Locking-condition-1"><span>Locking condition</span></a></li><li><a class="tocitem" href="#Cautious-BFGS-1"><span>Cautious BFGS</span></a></li><li><a class="tocitem" href="#Limited-memory-Riemannian-BFGS-1"><span>Limited-memory Riemannian BFGS</span></a></li><li><a class="tocitem" href="#Interface-1"><span>Interface</span></a></li><li><a class="tocitem" href="#Problem-and-Options-1"><span>Problem &amp; Options</span></a></li><li><a class="tocitem" href="#Literature-1"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="stochastic_gradient_descent.html">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="subgradient.html">Subgradient method</a></li><li><a class="tocitem" href="truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="trust_regions.html">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../functions/index.html">Introduction</a></li><li><a class="tocitem" href="../functions/bezier.html">Bézier curves</a></li><li><a class="tocitem" href="../functions/costs.html">Cost functions</a></li><li><a class="tocitem" href="../functions/differentials.html">Differentials</a></li><li><a class="tocitem" href="../functions/adjointdifferentials.html">Adjoint Differentials</a></li><li><a class="tocitem" href="../functions/gradients.html">Gradients</a></li><li><a class="tocitem" href="../functions/Jacobi_fields.html">Jacobi Fields</a></li><li><a class="tocitem" href="../functions/proximal_maps.html">Proximal Maps</a></li><li><a class="tocitem" href="../functions/manifold.html">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../helpers/data.html">Data</a></li><li><a class="tocitem" href="../helpers/errorMeasures.html">Error Measures</a></li><li><a class="tocitem" href="../helpers/exports.html">Exports</a></li></ul></li><li><a class="tocitem" href="../list.html">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href="quasi_Newton.html">Quasi-Newton</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="quasi_Newton.html">Quasi-Newton</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/quasi_Newton.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="quasiNewton-1"><a class="docs-heading-anchor" href="#quasiNewton-1">Riemannian quasi-Newton methods</a><a class="docs-heading-anchor-permalink" href="#quasiNewton-1" title="Permalink"></a></h1><p>The aim is to minimize a real-valued function on a Riemannian manifold, i.e.</p><div>\[\min f(x), \quad x \in \mathcal{M}.\]</div><p>Riemannian quasi-Newtonian methods are as generalizations of their Euclidean counterparts Riemannian line search methods. These methods first determine a search direction <span>$\eta_k$</span> in each iteration, which is a tangent vector in the tangent space <span>$T_{x_k} \mathcal{M}$</span> at the current iterate <span>$x_k$</span>, then serach for a suitable stepsize <span>$\alpha_k$</span> along the curve <span>$\gamma(\alpha) = R_{x_k}(\alpha \eta_k)$</span> which is determined by a chosen retractio <span>$R \colon T \mathcal{M} \to \mathcal{M}$</span> and the search direction <span>$\eta_k$</span>. The next iterate is obtained by</p><div>\[x_{k+1} = R_{x_k}(\alpha_k \eta_k).\]</div><p>The choice of a computationally efficient retraction is important, because it can influence the rate of convergence.  In quasi-Newton methods, the search direction is given by</p><div>\[\eta_k = -{\mathcal{H}_k}^{-1}[\operatorname{grad} f (x_k)] = -\mathcal{B}_k [\operatorname{grad} f (x_k)],\]</div><p>where <span>$\mathcal{H}_k \colon T_{x_k} \mathcal{M} \to T_{x_k} \mathcal{M}$</span> is a positive definite self-adjoint operator, which approximates the action of the Hessian <span>$\operatorname{Hess} f (x_k)[\cdot]$</span> and <span>$\mathcal{B}_k = {\mathcal{H}_k}^{-1}$</span>. The idea of quasi-Newton methods is instead of creating a complete new approximation of the Hessian operator <span>$\operatorname{Hess} f(x_{k+1})$</span> or its inverse at every iteration, the previous operator <span>$\mathcal{H}_k$</span> or <span>$\mathcal{B}_k$</span> is updated by a convenient formula using the obtained information about the curvature of the objective function during the iteration. The resulting operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> acts on the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span> of the freshly computed iterate <span>$x_{k+1}$</span>. In order to get a well-defined method, the following requirements are placed on the new operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> that is created by an update. Since the Hessian <span>$\operatorname{Hess} f(x_{k+1})$</span> is a self-adjoint operator on the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>, and <span>$\mathcal{H}_{k+1}$</span> approximates it, we require that <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> is also self-adjoint on <span>$T_{x_{k+1}} \mathcal{M}$</span>. In order to achieve a steady descent, we want <span>$\eta_k$</span> to be a descent direction in each iteration. Therefore we require, that <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> is a positive definite operator on <span>$T_{x_{k+1}} \mathcal{M}$</span>. In order to get information about the cruvature of the objective function into the new operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span>, we require that it satisfies a form of a Riemannian quasi-Newton equation:</p><div>\[\mathcal{H}_{k+1} [T_{x_k \rightarrow x_{k+1}}({R_{x_k}}^{-1}(x_{k+1}))] = \operatorname{grad} f(x_{k+1}) - T_{x_k \rightarrow x_{k+1}}(\operatorname{grad} f(x_k))\]</div><p>or </p><div>\[\mathcal{B}_{k+1} [\operatorname{grad} f(x_{k+1}) - T_{x_k \rightarrow x_{k+1}}(\operatorname{grad} f(x_k))] = T_{x_k \rightarrow x_{k+1}}({R_{x_k}}^{-1}(x_{k+1}))\]</div><p>where <span>$T_{x_k \rightarrow x_{k+1}} \colon T_{x_k} \mathcal{M} \to T_{x_{k+1}} \mathcal{M}$</span> and the chosen retraction <span>$R$</span> is the associated retraction of <span>$T$</span>.  The idea of Riemannian quasi-Newton methods is to generate an operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> which meets all these requirements by a convenient update formula. Thereby, the quasi-Newton update formulas for matrices known from the Euclidean case were generalised for the Riemannian setup. </p><h2 id="Operator-Updates-1"><a class="docs-heading-anchor" href="#Operator-Updates-1">Operator Updates</a><a class="docs-heading-anchor-permalink" href="#Operator-Updates-1" title="Permalink"></a></h2><p>There are many update formulas that pursue different goals and/or are based on different ideas. For this, the following general definitions and terms will be needed:</p><p>Of course, in any update one wants to take over the information already stored in <span>$\mathcal{H}_k$</span>, or <span>$\mathcal{B}_k$</span>, but this is an operator on <span>$T_{x_k} \mathcal{M}$</span>, which is in general not the same tangent space as <span>$T_{x_{k+1}} \mathcal{M}$</span> on which <span>$\mathcal{H}_{k+1}$</span>, or <span>$\mathcal{B}_{k+1}$</span>, operates. To overcome this obstacle, we introduce</p><div>\[\widetilde{\mathcal{H}}_k = T^{S}_{x_k, \alpha_k \eta_k} \circ \mathcal{H}_k \circ {T^{S}_{x_k, \alpha_k \eta_k}}^{-1} \colon T_{x_{k+1}} \mathcal{M} \to T_{x_{k+1}} \mathcal{M},\]</div><p>where <span>$T^{S}_{x_k, \alpha_k \eta_k} \colon T_{x_k} \mathcal{M} \to T_{R_{x_k}(\alpha_k \eta_k)} \mathcal{M}$</span> is an isometric vector transport and <span>$R_{x_k}(\cdot)$</span> is its associated retraction. Of course, one could take any vector transport <span>$T \colon T_{x_k} \mathcal{M} \to T_{x_{k+1}} \mathcal{M}$</span> to which <span>$R$</span> is the associated retraction, i.e. <span>$x_{k+1} = R_{x_k}(\alpha_k \eta_k) \in \mathcal{M}$</span>, but since we want to take on the positive definiteness and self-adjointness of the operator <span>$\mathcal{H}_k$</span>, this is in general only ensured by an isometric vector transport <span>$T^S$</span>. The same method is used to define <span>$\widetilde{\mathcal{B}}_k \colon T_{x_{k+1}} \mathcal{M} \to T_{x_{k+1}} \mathcal{M}$</span>.  To get the curvature information of the objective function into the update formula, the tangent vectors </p><div>\[s_k = T^{S}_{x_k, \alpha_k \eta_k}(\alpha_k \eta_k) \in T_{x_{k+1}} \mathcal{M}\]</div><p>and </p><div>\[y_k = \operatorname{grad} f(x_{k+1}) - T^{S}_{x_k, \alpha_k \eta_k}(\operatorname{grad} f(x_k)) \in T_{x_{k+1}} \mathcal{M}\]</div><p>are defined. Here, of course, the same vector transport as for <span>$\widetilde{\mathcal{H}}_k$</span>, or <span>$\widetilde{\mathcal{B}}_k$</span> is used. In Euclidean quasi-Newton methods, the property that <span>$s_k s_k^{\mathrm{T}}$</span> is a positive definite symmetric rank one matrix is used to construct matrix updates. For the generalisation we need the musical isomorphism <span>$\flat$</span>, which turns a tangent vector into a cotangent vector, i.e. </p><div>\[\flat \colon \; T_{x} \mathcal{M} \ni \xi_x \mapsto \xi^{\flat}_x \in T^{*}_{x} \mathcal{M}\]</div><p>and <span>$\xi^{\flat}_x \colon T_{x} \mathcal{M} \to T_{x} \mathcal{M}$</span> satisfies</p><div>\[\xi^{\flat}_x(\eta_x) = g_x(\xi_x,\eta_x),\]</div><p>where <span>$g_x \colon T_{x} \mathcal{M} \times T_{x} \mathcal{M} \to \mathbb{R}$</span> is the chosen Riemannian metric of the manifold <span>$\mathcal{M}$</span>. This allows one to define e.g. <span>$y_k y^{\flat}_k$</span>, which is a postive definite self-adjoint rank one operator on <span>$T_{x_k} \mathcal{M}$</span>.  With these definitions and terms, the following update formulas can now be defined. We note that all the methods described here satisfy a form of the Riemannian quasi-Newton equation.  The Riemannian BFGS update, also called RBFGS update, is a rank two operator update which turns a positive definite self-adjoint operator <span>$\mathcal{H}^{RBFGS}_k$</span> on <span>$T_{x_k} \mathcal{M}$</span> into a positive definite self-adjoint operator <span>$\mathcal{H}^{RBFGS}_{k+1}$</span> on <span>$T_{x_{k+1}} \mathcal{M}$</span>, if <span>$g_{x_{k+1}}(s_k, y_k) &gt; 0$</span> holds:</p><div>\[\mathcal{H}^{RBFGS}_{k+1} [\cdot] = \widetilde{\mathcal{H}}^{RBFGS}_k [\cdot] + \frac{y_k y^{\flat}_k[\cdot]}{s^{\flat}_k [y_k]} - \frac{\widetilde{\mathcal{H}}^{RBFGS}_k [s_k] s^{\flat}_k (\widetilde{\mathcal{H}}^{RBFGS}_k [\cdot])}{s^{\flat}_k (\widetilde{\mathcal{H}}^{RBFGS}_k [s_k])}.\]</div><p>Using the Sherman-Morrison-Woodbury formula for operators, we obtain the inverse RBFGS update, which also produces a positive definite self-adjoint operator <span>$\mathcal{B}^{RBFGS}_{k+1}$</span> if <span>$\mathcal{B}^{RBFGS}_k$</span> is positive definite and self-adjoint on <span>$T_{x_k} \mathcal{M}$</span> and <span>$g_{x_{k+1}}(s_k, y_k) &gt; 0$</span> holds:</p><div>\[\mathcal{B}^{RBFGS}_{k+1} [\cdot] = \Big{(} \id_{T_{x_{k+1}} \mathcal{M}}[\cdot] - \frac{s_k y^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} \widetilde{\mathcal{B}}^{RBFGS}_k [\cdot] \Big{(} \id_{T_{x_{k+1}} \mathcal{M}}[\cdot] - \frac{y_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} + \frac{s_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]}.\]</div><p>By replacing the triple <span>$(\widetilde{\mathcal{B}}^{RBFGS}_k, s_k, y_k)$</span> by the triple <span>$(\widetilde{\mathcal{H}}^{RDFP}_k, y_k, s_k)$</span> one obtains the Riemannian DFP update, short RDFP, which also generates a positive definite self-adjoint operator <span>$\mathcal{H}^{RDFP}_{k+1}$</span> on <span>$T_{x_{k+1}} \mathcal{M}$</span> if <span>$\mathcal{H}^{RDFP}_k$</span> is positive definite and self-adjoint and <span>$g_{x_{k+1}}(s_k, y_k) &gt; 0$</span> holds:</p><div>\[\mathcal{H}^{RDFP}_{k+1} [\cdot] = \Big{(} \id_{T_{x_{k+1}} \mathcal{M}}[\cdot] - \frac{y_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} \widetilde{\mathcal{H}}^{RDFP}_k [\cdot] \Big{(} \id_{T_{x_{k+1}} \mathcal{M}}[\cdot] - \frac{s_k y^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} + \frac{y_k y^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]}.\]</div><p>The RDFP and RBFGS update can be referred to as dual updates, similar to the Euclidean case, since the other update formula is obtained by exchanging the variables in one update formula. Therefore, one can also very quickly set up the inverse RDFP update, but it can also be achieved again by the Sherman-Morrison-Woodbury formula for operators:</p><div>\[\mathcal{B}^{RDFP}_{k+1} [\cdot] = \widetilde{\mathcal{B}}^{RDFP}_k [\cdot] + \frac{s_k s^{\flat}_k[\cdot]}{s^{\flat}_k [y_k]} - \frac{\widetilde{\mathcal{B}}^{RDFP}_k [y_k]  y^{\flat}_k (\widetilde{\mathcal{B}}^{RDFP}_k [\cdot])}{y^{\flat}_k (\widetilde{\mathcal{B}}^{RDFP}_k [y_k])}.\]</div><p>By the convex combination of the RBFGS and the RDFP updates one obtains the so-called Riemannian Broyden class, which under the same requirements as before produces a positive definite self-adjoint operator <span>$\mathcal{H}^{Broyden}_{k+1}$</span> on <span>$T_{x_{k+1}} \mathcal{M}$</span>. The factor <span>$\phi_k$</span> can either evolve in each iteration as desribed below, or it can also be fixed, i.e. <span>$\phi_k = \phi \in [0,1]$</span> for all <span>$k$</span>:</p><div>\[\mathcal{H}^{Broyden}_{k+1} [\cdot] = (1-\phi_k) \, \mathcal{H}^{RBFGS}_{k+1} [\cdot] + \phi_k \, \mathcal{H}^{RDFP}_{k+1} [\cdot]\]</div><p>where</p><div>\[\phi_k \in (\phi^{\mathrm{c}}_k, \infty), \; \phi^{\mathrm{c}}_k = \frac{1}{1 - u_k}, \; u_k = \frac{g_{x_{k+1}}(y_k, {\widetilde{\mathcal{H}}^{Broyden}_k}^{-1} [y_k]) g_{x_{k+1}}(s_k, \widetilde{\mathcal{H}}^{Broyden}_k [s_k])}{g_{x_{k+1}}(s_k, y_k)^2}.\]</div><p>Another Riemannian quasi-Newton update, which does not transfer the positive definiteness but the self-adjointness of the operator <span>$\mathcal{H}^{SR1}_k$</span>, is the generalised Symmetric Rank One update, or SR1 update for short. As the name suggests, this is a rank one operator update, which only ensures the preservation of symmetry, i.e. self-adjointness, of the operator <span>$\mathcal{H}^{SR1}_k$</span>:</p><div>\[\mathcal{H}^{RSR1}_{k+1} [\cdot] = \widetilde{\mathcal{H}}^{RSR1}_k [\cdot] + \frac{(y_k - \widetilde{\mathcal{H}}^{RSR1}_k [s_k])(y_k - \widetilde{\mathcal{H}}^{RSR1}_k [s_k])^{\flat}[\cdot]}{(y_k - \widetilde{\mathcal{H}}^{RSR1}_k [s_k])^{\flat}[s_k]}.\]</div><p>By replacing the triple <span>$(\widetilde{\mathcal{H}}^{RSR1}_k, s_k, y_k)$</span> by the triple <span>$(\widetilde{\mathcal{B}}^{SR1}_k, y_k, s_k)$</span> we get the inverse SR1 update:  </p><div>\[\mathcal{B}^{RSR1}_{k+1} [\cdot] = \widetilde{\mathcal{B}}^{RSR1}_k [\cdot] + \frac{(s_k - \widetilde{\mathcal{B}}^{RSR1}_k [y_k])(s_k - \widetilde{\mathcal{B}}^{RSR1}_k [y_k])^{\flat}[\cdot]}{(s_k - \widetilde{\mathcal{B}}^{RSR1}_k [y_k])^{\flat}[y_k]}.\]</div><h2 id="Initialization-1"><a class="docs-heading-anchor" href="#Initialization-1">Initialization</a><a class="docs-heading-anchor-permalink" href="#Initialization-1" title="Permalink"></a></h2><p>Initialize <span>$x_0 \in \mathcal{M}$</span> and let <span>$\mathcal{B}_0 \colon T_{x_0} \mathcal{M} \to T_{x_0} \mathcal{M}$</span> be a positive definite, self-adjoint operator.</p><h2 id="Iteration-1"><a class="docs-heading-anchor" href="#Iteration-1">Iteration</a><a class="docs-heading-anchor-permalink" href="#Iteration-1" title="Permalink"></a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Compute <span>$\eta_k = -\mathcal{B}_k [\operatorname{grad} f (x_k)]$</span> or solve <span>$\mathcal{H}_k [\eta_k] = -\operatorname{grad} f (x_k)]$</span>.</li><li>Determine a suitable stepsize <span>$\alpha_k$</span> along the curve given by <span>$\gamma(\alpha) = R_{x_k}(\alpha \eta_k)$</span> (e.g. by using the Riemannian Wolfe conditions).</li><li>Compute <span>$x_{k+1} = R_{x_k}(\alpha_k)$</span>.</li><li>Define <span>$s_k = T_{x_k, \alpha_k \eta_k}(\alpha_k \eta_k)$</span> and <span>$y_k = \operatorname{grad} f(x_{k+1}) - T_{x_k, \alpha_k \eta_k}(\operatorname{grad} f(x_k))$</span>.</li><li>Update <span>$\mathcal{B}_k \text{ or } \mathcal{H}_k \mapsto \mathcal{B}_{k+1} \text{ or } \mathcal{H}_{k+1} \colon T_{x_{k+1}} \mathcal{M} \to T_{x_{k+1}} \mathcal{M}$</span>.</li></ol><h2 id="Result-1"><a class="docs-heading-anchor" href="#Result-1">Result</a><a class="docs-heading-anchor-permalink" href="#Result-1" title="Permalink"></a></h2><p>The result is given by the last computed <span>$x_K$</span>.</p><h2 id="Locking-condition-1"><a class="docs-heading-anchor" href="#Locking-condition-1">Locking condition</a><a class="docs-heading-anchor-permalink" href="#Locking-condition-1" title="Permalink"></a></h2><p>Essential for the positive definiteness of the operators <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> for many methods is the fulfilment of </p><div>\[g_{x_{k+1}}(s_k, y_k) &gt; 0,\]</div><p>which is the so-called Riemannian curvature condition. If this condition holds in each iteration, the corresponding update, which uses an isometric vector transport <span>$T^S$</span>, produces a sequence of positive definite self-adjoint operators <span>$\{ \mathcal{H}_k \}_k$</span> or <span>$\{ \mathcal{B}_k \}_k$</span>. The same association exists, of course, in the Euclidean case, where the fulfilment of the curvature condition <span>$s^{\mathrm{T}}_k y_k$</span> is ensured by choosing a stepsize <span>$\alpha_k &gt; 0$</span> in each ietartion that fulfils the Wolfe conditions. The generalisation of the Wolfe conditions for Riemannian manifolds is that the stepsize <span>$\alpha_k &gt; 0$</span> satisfies </p><div>\[f( R_{x_k}(\alpha_k \eta_k)) \leq f(x_k) + c_1 \alpha_k g_{x_k} (\operatorname{grad} f(x_k), \eta_k)\]</div><p>and </p><div>\[\frac{\mathrm{d}}{\mathrm{d}t} f(R_{x_k}(t \; \eta_k)) \vert_{t=\alpha_k} \geq c_2 \frac{\mathrm{d}}{\mathrm{d}t} f(R_{x_k}(t \; \eta_k)) \vert_{t=0}.\]</div><p>Using the vector transport by differentiated retraction <span>$T^{R}{x, \eta_x}(\xi_x) = \frac{\mathrm{d}}{\mathrm{d}t} R_{x}(\eta_x + t \; \xi_x) \; \vert_{t=0}$</span>, where the retraction <span>$R$</span> is the associated retraction of the vector transport used in the update formula, the second condition can be rewritten as</p><div>\[g_{R_{x_k}(\alpha_k \eta_k)} (\operatorname{grad} f(R_{x_k}(\alpha_k \eta_k)), T^{R}{x_k, \alpha_k \eta_k}(\eta_k)) \geq c_2 g_{x_k} (\operatorname{grad} f(x_k), \eta_k).\]</div><p>Unfortunately, in general, a stepsize <span>$\alpha_k &gt; 0$</span> that satisfies the Riemannian Wolfe conditions and an isometric vector transport <span>$T^S$</span> in the update formula does not in general lead to a positive definite update of the operator <span>$\mathcal{H}_k$</span> or <span>$\mathcal{B}_k$</span>. This is because the Wolfe conditions use vector transport by differentiated retraction <span>$T^R$</span>, which is generally not the same as isometric vetor transport <span>$T^S$</span> used in the update. However, in order to create a positive definite operator in each iteration, the so-called locking condition was introduced, which requires that the isometric vector transport <span>$T^S$</span> and its associate retraction <span>$R$</span> fulfil the following condition</p><div>\[T^{S}{x, \xi_x}(\xi_x) = \beta T^{R}{x, \xi_x}(\xi_x), \quad \beta = \frac{\lVert \xi_x \rVert_x}{\lVert T^{R}{x, \xi_x}(\xi_x) \rVert_{R_{x}(\xi_x)}}.\]</div><p>where <span>$T^R$</span> is again the vector transport by differentiated retraction. With the requirement that the isometric vector transport <span>$T^S$</span> and its associared retraction <span>$R$</span> satisfies the locking condition and using the tangent vector </p><div>\[y_k = {\beta_k}^{-1} \operatorname{grad} f(x_{k+1}) - T^{S}{x_k, \alpha_k \eta_k}(\operatorname{grad} f(x_k)),\]</div><p>where </p><div>\[\beta_k = \frac{\lVert \alpha_k \eta_k \rVert_{x_k}}{\lVert \lVert T^{R}{x_k, \alpha_k \eta_k}(\alpha_k \eta_k) \rVert_{x_{k+1}}},\]</div><p>in the update, it can be shown that choosing a stepsize <span>$\alpha_k &gt; 0$</span> that satisfies the Riemannian wolfe conditions leads to the fulfilment of the Riemannian curvature condition, which in turn implies that the operator generated by the updates is positive definite. </p><h2 id="Cautious-BFGS-1"><a class="docs-heading-anchor" href="#Cautious-BFGS-1">Cautious BFGS</a><a class="docs-heading-anchor-permalink" href="#Cautious-BFGS-1" title="Permalink"></a></h2><p>As in the Euclidean case, the Riemannian BFGS method is not globally convergent for general objective functions. To mitigate this obstacle, the cautious update has been generalised to Riemannian manifolds. For that, in each iteration, based on a decision rule, either the usual update is used or the current operator is transported into the upcoming tangent, which in turn means that the update consists of the vector transport of B only. In summary, this can be expressed as follows:</p><div>\[\mathcal{B}^{CRBFGS}_{k+1} = \begin{cases} \text{using \cref{RiemannianInverseBFGSFormula}}, &amp; \; \frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \geq \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k}), \\ \widetilde{\mathcal{B}}^{CRBFGS}_k, &amp; \; \text{otherwise}, \end{cases}\]</div><h2 id="Limited-memory-Riemannian-BFGS-1"><a class="docs-heading-anchor" href="#Limited-memory-Riemannian-BFGS-1">Limited-memory Riemannian BFGS</a><a class="docs-heading-anchor-permalink" href="#Limited-memory-Riemannian-BFGS-1" title="Permalink"></a></h2><p>As in the Euclidean case, for manifolds of very large dimensions, both the memory required and the computationally cost can be extremely high for quasi-Newton methods, since in each iteration an operator on the tangent space with the same dimension of the manifold must be transported and stored. To overcome this obstacle, the limited-memory BFGS method was generalised for the Riemannian setup.  At the beginning of the k-th iteration, the search direction <span>$\eta_k = \mathcal{B}^{LRBFGS}_k [\operatorname{grad}f(x_k)]$</span> is calculated using the two-loop recursion with <span>$m$</span> (<span>$&lt;$</span> dimension of the manifold) stored tangent vectors <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1} \subset T_{x_k} \mathcal{M}$</span> and a positive definite selfadjoint operator <span>$\mathcal{B}^{(0)}_k \colon T_{x_k} \mathcal{M} \to T_{x_k} \mathcal{M}$</span> that varies from iteration to iteration:</p><p>\begin{algorithm}[H] 	\begin{algorithmic}[1]         \State <span>$q = \operatorname{grad} f(x_k)$</span></p><pre><code class="language-none">    \For{$i = k-1, k-2, \cdots, k-m$}
        \State $\rho_i = \frac{1}{g_{x_k}(\widetilde{s}_i, \widetilde{y}_i)}$
        \State $\xi_i = \rho_i g_{x_k}(\widetilde{s}_i, q)$ 
        \State $q = q - \xi_i \widetilde{y}_i$
    \EndFor

    \State $r = \mathcal{B}^{(0)}_k[q]$
    
    \For{$i = k-m, k-m+1, \cdots, k-1$}
        \State $\omega = \rho_i g_{x_k}(\widetilde{y}_i, r)$ 
        \State $r= r  + (\xi_i - \omega) \widetilde{s}_i$
	\EndFor
	
	\State \textbf{Stop with result} $\mathcal{B}^{LRBFGS}_k[\operatorname{grad} f(x_k)]$.
\end{algorithmic}</code></pre><p>\end{algorithm}</p><p>This algorithm can be understood that the usual RBFGS update is executed <span>$m$</span> times in a row on <span>$\mathcal{B}^{(0)}_k$</span> and is directly applied on <span>$\operatorname{grad}f(x_k)$</span>. Therefore, only inner products and linear combinations in the tangent space are needed. We note that the resulting operator <span>$\mathcal{B}^{LRBFGS}_k$</span> is an approximation of the operator <span>$\mathcal{B}^{RBFGS}_k$</span>, since for k&gt;m the information of all previous iterations is not included in the operator (unless of course one chooses m as large as the maximum number of iterations).</p><p>The operator <span>$\mathcal{B}^{(0)}_k$</span> can be chosen to be fixed in each iteration, but the principle known to be successful from the Euclidean case can easily be generalised for the Riemannian setup by choosing <span>$\mathcal{B}^{(0)}_k[\cdot] = c_k \id_{T_{x_k} \mathcal{M}}[\cdot],$</span> where</p><div>\[
c_k = \frac{\widetilde{s}^{\flat}_{k-1} \widetilde{y}_{k-1}}{\widetilde{y}^{\flat}_{k-1} \widetilde{y}_{k-1}} = \frac{s^{\flat}_{k-1} y_{k-1}}{y^{\flat}_{k-1} y_{k-1}} = \frac{g_{x_k}(s_{k-1}, y_{k-1})}{g_{x_k}(y_{k-1}, y_{k-1})}.
$

For the first iteration usually the identity operator is used. In the $k\]</div><p>-th iteration, the next iterate is calculated as usual, i.e. <span>$x_{k+1} = R_{x_k}(\alpha_k \eta_k)$</span>, where alpha is a step size that satisfies the wolfe conditions. Then comes the crux of the algortihmus, which has a memory advantage over the usual quasi-Newton methods. When updating, the oldest vector pair <span>$\{ \widetilde{s}_{k−m}, \widetilde{y}_{k−m}\}$</span> of the set <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span> is exchanged with the newest vector pair <span>$\{ \widetilde{s}_k, \widetilde{y}_k\}$</span>. There are two cases: if there is still free memory, i.e. <span>$k &lt; m$</span>, the previously stored vector pairs <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span> have to be transported into the new tangent space <span>$T_{x_k} \mathcal{M}$</span>; if there is no free memory, the oldest pair has to be discarded and then all the remaining vector pairs are transported into the new tangent space. This method ensures that all tangent vectors are in the correct tangent space, i.e. T, so that in the next iteration the search direction can be recursively calculated again. After that we calculate and store s and y . This ensures that new information about the target function is always included and the old, probably no longer relevant, information is discarded. </p><p>In summary, the algorithm takes the following form:</p><h2 id="Interface-1"><a class="docs-heading-anchor" href="#Interface-1">Interface</a><a class="docs-heading-anchor-permalink" href="#Interface-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.quasi_Newton" href="#Manopt.quasi_Newton"><code>Manopt.quasi_Newton</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">quasi_Newton(M, F, ∇F, x)</code></pre><p>evaluate a Riemannian quasi-Newton solver for optimization on manifolds. It will attempt to minimize the cost function F on the Manifold M.</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal{M}$</span>.</li><li><code>F</code> – a cost function <span>$F \colon \mathcal{M} \to \mathbb{R}$</span> to minimize.</li><li><code>∇F</code>– the gradient <span>$\nabla F \colon \mathcal M \to \tangent{x}$</span> of <span>$F$</span>.</li><li><code>x</code> – an initial value <span>$x \in \mathcal{M}$</span>.</li></ul><p><strong>Optional</strong></p><ul><li><code>retraction_method</code> – (<code>ExponentialRetraction()</code>) a retraction method to use, by default the exponntial map.</li><li><code>vector_transport_method</code> – (<code>ParallelTransport()</code>) a vector transport to use, by default the parallel transport.</li><li><code>broyden_factor</code> – (<code>0.</code>) – specifies the proportion of DFP update in the convex combination if the Broyden Class method is to be used. By default, BFGS is used.</li><li><code>cautious_update</code> – (<code>false</code>) – specifies whether a cautious update should be used, which means that a decision rule based on the calculated values decides whether the operator remains the same and no new information is received, or whether it is updated as usual.</li><li><code>cautious_function</code> – (<code>(x) -&gt; x*10^(-4)</code>) – a monotone increasing function that is zero at 0 and strictly increasing at 0.</li><li><code>memory_size</code> – (<code>20</code>) – number of vectors to be stored.</li><li><code>memory_steps</code>– (<code>[</code><a href="solvers/@ref"><code>zero_tangent_vector</code></a><code>(M,x) for _ ∈ 1:memory_size]</code>) – the currently stored tangent vectors <span>$s_k$</span> for a LRBFGS method.</li><li><code>memory_gradients</code> – (<code>[</code><a href="solvers/@ref"><code>zero_tangent_vector</code></a><code>(M,x) for _ ∈ 1:memory_size]</code>) – the currently stored tangent vectors <span>$y_k$</span> for a LRBFGS method.</li><li><code>initial_operator</code> – (<code>Matrix(I, [</code>manifold<em>dimension<code>](@ref)</code>(M), [`manifold</em>dimension<code>](@ref)</code>(M))`) – the initial operator, represented as a matrix.</li><li><code>scalling_initial_operator</code> – (<code>true</code>) specifies if the initial operator is scalled after the first step but before the first update.</li><li><code>step_size</code> – (<a href="../plans/index.html#Manopt.WolfePowellLineseach"><code>WolfePowellLineseach</code></a><code>(retraction_method, vector_transport_method)</code>) specify a <a href="../plans/index.html#Manopt.Stepsize"><code>Stepsize</code></a> functor.</li><li><code>stopping_criterion</code>– (<a href="index.html#Manopt.StopWhenAny"><code>StopWhenAny</code></a><code>(</code><a href="index.html#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(max(1000, memory_size)),</code><a href="index.html#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(10.0^68))</code>)</li><li><code>return_options</code> – (<code>false</code>) – if activated, the extended result, i.e. the   complete <a href="../plans/index.html#Manopt.Options"><code>Options</code></a> are returned. This can be used to access recorded values.   If set to false (default) just the optimal value <code>x_opt</code> if returned</li></ul><p><strong>Output</strong></p><ul><li><code>x_opt</code> – the resulting (approximately critical) point of the quasi–Newton method</li></ul><p>OR</p><ul><li><code>options</code> – the options returned by the solver (see <code>return_options</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/ebc5f33e43d7f3cb63d3b579a103141eda93eb87/src/solvers/quasi_Newton.jl#L1-L35">source</a></section></article><h2 id="Problem-and-Options-1"><a class="docs-heading-anchor" href="#Problem-and-Options-1">Problem &amp; Options</a><a class="docs-heading-anchor-permalink" href="#Problem-and-Options-1" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>GradientProblem</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="Manopt.QuasiNewtonOptions" href="#Manopt.QuasiNewtonOptions"><code>Manopt.QuasiNewtonOptions</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">QuasiNewtonOptions &lt;: Options</code></pre><p>Theese Quasi Newton <a href="../plans/index.html#Manopt.Options"><code>Options</code></a> represent any quasi newton based method and can be used with any update rule for the direction.</p><p><strong>Fields</strong></p><ul><li><code>x</code> – the current iterate, a point on a manifold</li><li><code>∇</code> – the current gradient</li><li><code>sk</code> – the current step</li><li><code>yk</code> the current gradient difference</li><li><code>direction_update</code> - a [<code>AbstractQuasiNewtonDirectionUpdate</code>] rule.</li><li><code>retraction_method</code> – a function to perform a step on the manifold</li><li><code>stop</code> – a <a href="index.html#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li></ul><p><strong>See also</strong></p><p><a href="../plans/index.html#Manopt.GradientProblem"><code>GradientProblem</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/ebc5f33e43d7f3cb63d3b579a103141eda93eb87/src/plans/gradient_plan.jl#L1083-L1100">source</a></section></article><h2 id="Literature-1"><a class="docs-heading-anchor" href="#Literature-1">Literature</a><a class="docs-heading-anchor-permalink" href="#Literature-1" title="Permalink"></a></h2></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="particle_swarm.html">« Particle Swarm Optimization</a><a class="docs-footer-nextpage" href="stochastic_gradient_descent.html">Stochastic Gradient Descent »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 13 January 2021 20:57">Wednesday 13 January 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
